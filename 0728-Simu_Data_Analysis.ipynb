{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Results Analyzer\n",
    "\n",
    "This notebook analyzes existing saved simulation results from the simulation runner script. It loads all per-run metrics and posterior summaries to generate aggregated summary plots and LaTeX tables for the manuscript.\n",
    "\n",
    "**Functionality:**\n",
    "- Loads all per-run metrics (`.json` files).\n",
    "- Loads posterior summaries (`.npz` files) for each run.\n",
    "- Loads benchmark results (`.npz` files) for each run.\n",
    "- Aggregates the time-series data to create average factual and counterfactual curves for each scenario.\n",
    "- Generates and saves summary plots:\n",
    "    - A 4x3 grid plot for factual CFR estimates.\n",
    "    - A 4x3 grid plot for counterfactual CFR estimates.\n",
    "    - Box plots for all evaluation metrics (MAE, MCIW, Coverage, Bias, etc.).\n",
    "- Aggregates scalar metrics and produces LaTeX summary tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import importlib\n",
    "\n",
    "# Import and reload custom modules to ensure the latest versions are used\n",
    "import config; importlib.reload(config)\n",
    "import data_generation; importlib.reload(data_generation)\n",
    "import benchmarks; importlib.reload(benchmarks)\n",
    "import evaluation; importlib.reload(evaluation)\n",
    "import plotting; importlib.reload(plotting)\n",
    "import tables; importlib.reload(tables)\n",
    "import results_io; importlib.reload(results_io)\n",
    "\n",
    "print(\"All modules imported and reloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper and Main Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_metrics_dataframe(df):\n",
    "    \"\"\"Cleans a DataFrame by converting list-like values in object columns to scalars.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            is_list_like = df[col].notna().any() and isinstance(df[col].dropna().iloc[0], list)\n",
    "            if is_list_like:\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: x[0] if isinstance(x, list) and len(x) == 1 else (np.nan if isinstance(x, list) else x)\n",
    "                ).astype(float, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def prepare_aggregated_plot_data(results_df_all):\n",
    "    \"\"\"Aggregates time-series results from all valid MC runs for the summary plots.\"\"\"\n",
    "    aggregated_plot_data_list = []\n",
    "    study_global_seed = config.GLOBAL_BASE_SEED\n",
    "\n",
    "    for scenario_idx, scenario_config in enumerate(tqdm(config.SCENARIOS, desc=\"Aggregating Plot Data\")):\n",
    "        scenario_id = scenario_config[\"id\"]\n",
    "        scenario_base_seed = study_global_seed + (scenario_idx * config.NUM_MONTE_CARLO_RUNS * 1000)\n",
    "        \n",
    "        # Regenerate true data for one run to get the ground truth curves\n",
    "        sim_data_true = data_generation.simulate_scenario_data(scenario_config, run_seed=scenario_base_seed)\n",
    "        T_analyze = config.T_ANALYSIS_LENGTH\n",
    "\n",
    "        scen_df_valid = results_df_all[(results_df_all[\"scenario_id\"] == scenario_id) & (results_df_all[\"error\"].isin([None, \"None\"]))]\n",
    "        if scen_df_valid.empty: continue\n",
    "        \n",
    "        # Initialize lists to collect time-series data from all valid runs\n",
    "        series_data = {key: [] for key in ['sCFR_mean', 'sCFR_lower', 'sCFR_upper', \n",
    "                                           'sCFR_cf_mean', 'sCFR_cf_lower', 'sCFR_cf_upper',\n",
    "                                           'cCFR_mean', 'cCFR_lower', 'cCFR_upper', \n",
    "                                           'aCFR_mean', 'aCFR_lower', 'aCFR_upper',\n",
    "                                           'ITS_factual_mean', 'ITS_factual_lower', 'ITS_factual_upper',\n",
    "                                           'ITS_cf_mean', 'ITS_cf_lower', 'ITS_cf_upper']}\n",
    "\n",
    "        for mc_run_idx in scen_df_valid[\"mc_run\"].astype(int) - 1:\n",
    "            posterior_summary = results_io.load_posterior_summary_for_run(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SUMMARIES)\n",
    "            if posterior_summary:\n",
    "                series_data['sCFR_mean'].append(posterior_summary.get(\"p_mean\", []))\n",
    "                series_data['sCFR_lower'].append(posterior_summary.get(\"p_q025\", []))\n",
    "                series_data['sCFR_upper'].append(posterior_summary.get(\"p_q975\", []))\n",
    "                series_data['sCFR_cf_mean'].append(posterior_summary.get(\"p_cf_mean\", []))\n",
    "                series_data['sCFR_cf_lower'].append(posterior_summary.get(\"p_cf_q025\", []))\n",
    "                series_data['sCFR_cf_upper'].append(posterior_summary.get(\"p_cf_q975\", []))\n",
    "\n",
    "            benchmark_results = results_io.load_benchmark_results(scenario_id, mc_run_idx, config.OUTPUT_DIR_BENCHMARK_RESULTS)\n",
    "            if benchmark_results:\n",
    "                series_data['cCFR_mean'].append(benchmark_results.get(\"cCFR_cumulative\", []))\n",
    "                series_data['cCFR_lower'].append(benchmark_results.get(\"cCFR_cumulative_lower\", []))\n",
    "                series_data['cCFR_upper'].append(benchmark_results.get(\"cCFR_cumulative_upper\", []))\n",
    "                series_data['aCFR_mean'].append(benchmark_results.get(\"aCFR_cumulative\", []))\n",
    "                series_data['aCFR_lower'].append(benchmark_results.get(\"aCFR_cumulative_lower\", []))\n",
    "                series_data['aCFR_upper'].append(benchmark_results.get(\"aCFR_cumulative_upper\", []))\n",
    "                series_data['ITS_factual_mean'].append(benchmark_results.get(\"its_factual_mean\",[]))\n",
    "                series_data['ITS_factual_lower'].append(benchmark_results.get(\"its_factual_lower\",[]))\n",
    "                series_data['ITS_factual_upper'].append(benchmark_results.get(\"its_factual_upper\",[]))\n",
    "                series_data['ITS_cf_mean'].append(benchmark_results.get(\"its_counterfactual_mean\",[]))\n",
    "                series_data['ITS_cf_lower'].append(benchmark_results.get(\"its_counterfactual_lower\",[]))\n",
    "                series_data['ITS_cf_upper'].append(benchmark_results.get(\"its_counterfactual_upper\",[]))\n",
    "\n",
    "        # Calculate the point-wise average of the curves and intervals\n",
    "        agg_plot_dict = {\n",
    "            \"scenario_id\": scenario_id,\n",
    "            \"true_r_t\": sim_data_true[\"true_r_0_t\"][:T_analyze],\n",
    "            \"true_rcf_0_t\": sim_data_true[\"true_rcf_0_t\"][:T_analyze],\n",
    "            \"true_intervention_times_0_abs\": sim_data_true[\"true_intervention_times_0_abs\"],\n",
    "            \"estimated_r_t_dict\": {\n",
    "                \"sCFR\": {k.replace('sCFR_', ''): np.mean([s for s in series_data[k] if len(s)>0], axis=0)[:T_analyze] for k in series_data if 'sCFR' in k},\n",
    "                \"cCFR_cumulative\": {k.replace('cCFR_', ''): np.mean(series_data[k], axis=0)[:T_analyze] for k in series_data if 'cCFR' in k},\n",
    "                \"aCFR_cumulative\": {k.replace('aCFR_', ''): np.mean(series_data[k], axis=0)[:T_analyze] for k in series_data if 'aCFR' in k},\n",
    "                \"ITS_MLE\": {k.replace('ITS_', ''): np.mean(series_data[k], axis=0)[:T_analyze] for k in series_data if 'ITS' in k}\n",
    "            }\n",
    "        }\n",
    "        aggregated_plot_data_list.append(agg_plot_dict)\n",
    "\n",
    "    return aggregated_plot_data_list\n",
    "\n",
    "def main_analysis():\n",
    "    \"\"\"Main function to orchestrate the post-hoc analysis of simulation results.\"\"\"\n",
    "    all_loaded_metrics = []\n",
    "    print(\"Starting analysis of existing simulation results...\")\n",
    "    \n",
    "    for dir_path in [config.OUTPUT_DIR_PLOTS, config.OUTPUT_DIR_TABLES, config.OUTPUT_DIR_RESULTS_CSV]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Load all saved metrics from JSON files\n",
    "    for scenario in tqdm(config.SCENARIOS, desc=\"Loading All Metrics\"):\n",
    "        for mc_run in range(config.NUM_MONTE_CARLO_RUNS):\n",
    "            run_metrics = results_io.load_run_metrics(scenario[\"id\"], mc_run, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "            if run_metrics: all_loaded_metrics.append(run_metrics)\n",
    "    \n",
    "    if not all_loaded_metrics:\n",
    "        print(\"No metrics files found. Cannot generate plots or tables.\")\n",
    "        return\n",
    "        \n",
    "    results_df_all = pd.DataFrame(all_loaded_metrics)\n",
    "    results_df_valid = sanitize_metrics_dataframe(results_df_all)\n",
    "    results_df_valid = results_df_valid[results_df_valid['error'].isin([None, \"None\"])].copy()\n",
    "    \n",
    "    if results_df_valid.empty:\n",
    "        print(\"No valid simulation runs found. Analysis cannot proceed.\")\n",
    "        return\n",
    "    \n",
    "    # Aggregate scalar metrics (mean and std) for tables\n",
    "    cover_cols = [col for col in results_df_valid.columns if 'cover' in col]\n",
    "    for col in cover_cols: results_df_valid[col] = results_df_valid[col].astype('Int64')\n",
    "\n",
    "    summary_mean = results_df_valid.groupby(\"scenario_id\").mean(numeric_only=True).add_suffix('_mean').reset_index().rename(columns={'scenario_id_mean':'scenario_id'})\n",
    "    summary_std = results_df_valid.groupby(\"scenario_id\").std(numeric_only=True).add_suffix('_std').reset_index().rename(columns={'scenario_id_std':'scenario_id'})\n",
    "    results_df_summary = pd.merge(summary_mean, summary_std, on=\"scenario_id\", how=\"left\")\n",
    "    \n",
    "    analysis_csv_path = os.path.join(config.OUTPUT_DIR_RESULTS_CSV, \"all_scenarios_metrics_aggregated.csv\")\n",
    "    results_df_summary.to_csv(analysis_csv_path, index=False)\n",
    "    print(f\"\\nAggregated summary metrics saved to {analysis_csv_path}\")\n",
    "\n",
    "    # --- Generate Plots and Tables ---\n",
    "    print(\"\\nPreparing aggregated data for summary plots...\")\n",
    "    aggregated_plot_data = prepare_aggregated_plot_data(results_df_all)\n",
    "    \n",
    "    print(\"Generating aggregated factual summary plot...\")\n",
    "    plotting.plot_aggregated_factual_summary(aggregated_plot_data, config.OUTPUT_DIR_PLOTS)\n",
    "    \n",
    "    print(\"Generating aggregated counterfactual summary plot...\")\n",
    "    plotting.plot_aggregated_counterfactual_summary(aggregated_plot_data, config.OUTPUT_DIR_PLOTS)\n",
    "    \n",
    "    print(\"Generating summary boxplots...\")\n",
    "    plotting.plot_metric_summary_boxplots(results_df_valid, config.OUTPUT_DIR_PLOTS)\n",
    "    plotting.plot_combined_metrics_summary(results_df_valid, config.OUTPUT_DIR_PLOTS)\n",
    "    \n",
    "    print(\"Generating LaTeX summary tables...\")\n",
    "    tables.generate_rt_metrics_table(results_df_summary, config.OUTPUT_DIR_TABLES)\n",
    "    tables.generate_param_metrics_table(results_df_summary, config.OUTPUT_DIR_TABLES)\n",
    "\n",
    "    print(\"\\nAnalysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execute Analysis\n",
    "\n",
    "Run the cell below to perform the analysis. Make sure the simulation output directories in `config.py` point to where your simulation results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
