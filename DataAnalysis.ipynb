{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc5e68c",
   "metadata": {},
   "source": [
    "# Simulation Results Analyzer\n",
    "\n",
    "This notebook analyzes existing saved simulation results from the `run_simulation_notebook.ipynb` (or its script equivalent).\n",
    "\n",
    "**Functionality:**\n",
    "- Loads per-run metrics (`.json` files) from `OUTPUT_DIR_RUN_METRICS_JSON`.\n",
    "- Loads posterior summaries (mean, quantiles as `.npz` files) for the first MC run of each scenario from `OUTPUT_DIR_POSTERIOR_SUMMARIES`.\n",
    "- Regenerates true data (`sim_data`) for the first MC run of each scenario using stored seeds to provide context for plots.\n",
    "- Generates and saves individual time-series plots for the first MC run of each scenario (to `OUTPUT_DIR_PLOTS`).\n",
    "- Generates and saves a combined 4x3 grid plot of these time-series (to `OUTPUT_DIR_PLOTS`).\n",
    "- Generates and saves summary box plots for all evaluation metrics across all MC runs (to `OUTPUT_DIR_PLOTS`).\n",
    "- Aggregates metrics and produces LaTeX summary tables (to `OUTPUT_DIR_TABLES`).\n",
    "\n",
    "**Prerequisites:**\n",
    "1. The simulation runner notebook/script must have completed, and its output files must be available in the directories specified in `config.py`.\n",
    "2. All helper Python modules (`config.py`, `data_generation.py`, etc.) must be in the same directory or accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcecdf1",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ef1db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules have been imported and reloaded.\n",
      "Reading configuration and output paths from config.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import importlib\n",
    "\n",
    "# Import and force-reload all your custom modules to ensure the latest versions are used\n",
    "import config; importlib.reload(config)\n",
    "import data_generation; importlib.reload(data_generation)\n",
    "import benchmarks; importlib.reload(benchmarks)\n",
    "import evaluation; importlib.reload(evaluation)\n",
    "import plotting; importlib.reload(plotting)\n",
    "import tables; importlib.reload(tables)\n",
    "import results_io; importlib.reload(results_io)\n",
    "\n",
    "print(\"All modules have been imported and reloaded.\")\n",
    "print(\"Reading configuration and output paths from config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f6cc8",
   "metadata": {},
   "source": [
    "## 2. Main Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff36c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_metrics_dataframe(df):\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame of metrics by converting list-like values to scalars.\n",
    "    This is a safeguard against malformed metric files from older runs.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            # Check if the column contains list-like objects and flatten them\n",
    "            if df[col].notna().any() and isinstance(df[col].dropna().iloc[0], list):\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda x: x[0] if isinstance(x, list) and len(x) == 1 else (np.nan if isinstance(x, list) and len(x) == 0 else x)\n",
    "                ).astype(float, errors='ignore')\n",
    "    return df\n",
    "\n",
    "def prepare_aggregated_plot_data(results_df_all):\n",
    "    \"\"\"\n",
    "    Aggregates time-series results from all valid MC runs for the summary plot.\n",
    "    This version prepares data for sCFR-O (as requested for plotting) and the benchmarks.\n",
    "    \"\"\"\n",
    "    aggregated_plot_data_list = []\n",
    "    study_global_seed = config.GLOBAL_BASE_SEED\n",
    "\n",
    "    for scenario_idx, scenario_config_dict in enumerate(tqdm(config.SCENARIOS, desc=\"Aggregating Plot Data\")):\n",
    "        scenario_id = scenario_config_dict[\"id\"]\n",
    "        scenario_base_seed = study_global_seed + (scenario_idx * config.NUM_MONTE_CARLO_RUNS * 1000)\n",
    "        \n",
    "        sim_data_true = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=scenario_base_seed)\n",
    "        T_analyze = config.T_ANALYSIS_LENGTH\n",
    "\n",
    "        scen_df_valid = results_df_all[(results_df_all[\"scenario_id\"] == scenario_id) & (results_df_all[\"error\"].isin([None, \"None\"]))]\n",
    "        if scen_df_valid.empty: continue\n",
    "        \n",
    "        # Stacks for collecting time-series data from all valid runs\n",
    "        sCFR_means, sCFR_lowers, sCFR_uppers, sCFR_cf_means, sCFR_cf_lowers, sCFR_cf_uppers = [], [], [], [], [], []\n",
    "        cCFR_means, cCFR_lowers, cCFR_uppers = [], [], []\n",
    "        aCFR_means, aCFR_lowers, aCFR_uppers = [], [], []\n",
    "        ITS_factual_means, ITS_factual_lowers, ITS_factual_uppers = [], [], []\n",
    "        ITS_cf_means, ITS_cf_lowers, ITS_cf_uppers = [], [], []\n",
    "\n",
    "        for mc_run_idx in scen_df_valid[\"mc_run\"].astype(int) - 1:\n",
    "            # Load posterior summary for the superior sCFR-O model for plotting\n",
    "            posterior_summary = results_io.load_posterior_summary_for_run(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SUMMARIES)\n",
    "            if posterior_summary:\n",
    "                sCFR_means.append(posterior_summary.get(\"p_mean\", []))\n",
    "                sCFR_lowers.append(posterior_summary.get(\"p_q025\", []))\n",
    "                sCFR_uppers.append(posterior_summary.get(\"p_q975\", []))\n",
    "                sCFR_cf_means.append(posterior_summary.get(\"p_cf_mean\", []))\n",
    "                sCFR_cf_lowers.append(posterior_summary.get(\"p_cf_q025\", []))\n",
    "                sCFR_cf_uppers.append(posterior_summary.get(\"p_cf_q975\", []))\n",
    "\n",
    "            # Regenerate data to calculate benchmarks for this run\n",
    "            benchmark_results = results_io.load_benchmark_results(\n",
    "                scenario_id, mc_run_idx, config.OUTPUT_DIR_BENCHMARK_RESULTS\n",
    "            )\n",
    "            if benchmark_results:\n",
    "                cCFR_means.append(benchmark_results.get(\"cCFR_cumulative\", []))\n",
    "                cCFR_lowers.append(benchmark_results.get(\"cCFR_cumulative_lower\", []))\n",
    "                cCFR_uppers.append(benchmark_results.get(\"cCFR_cumulative_upper\", []))\n",
    "                \n",
    "                aCFR_means.append(benchmark_results.get(\"aCFR_cumulative\", []))\n",
    "                aCFR_lowers.append(benchmark_results.get(\"aCFR_cumulative_lower\", []))\n",
    "                aCFR_uppers.append(benchmark_results.get(\"aCFR_cumulative_upper\", []))\n",
    "\n",
    "                ITS_factual_means.append(benchmark_results.get(\"its_factual_mean\",[]))\n",
    "                ITS_factual_lowers.append(benchmark_results.get(\"its_factual_lower\",[]))\n",
    "                ITS_factual_uppers.append(benchmark_results.get(\"its_factual_upper\",[]))\n",
    "                ITS_cf_means.append(benchmark_results.get(\"its_counterfactual_mean\",[]))\n",
    "                ITS_cf_lowers.append(benchmark_results.get(\"its_counterfactual_lower\",[]))\n",
    "                ITS_cf_uppers.append(benchmark_results.get(\"its_counterfactual_upper\",[]))\n",
    "\n",
    "        # Calculate the point-wise average of the curves and intervals\n",
    "        agg_plot_dict = {\n",
    "            \"scenario_id\": scenario_id,\n",
    "            \"true_r_t\": sim_data_true[\"true_r_0_t\"][:T_analyze],\n",
    "            \"true_rcf_0_t\": sim_data_true[\"true_rcf_0_t\"][:T_analyze],\n",
    "            \"estimated_r_t_dict\": {\n",
    "                \"sCFR\": {\n",
    "                    \"mean\": np.mean([s for s in sCFR_means if len(s)>0], axis=0)[:T_analyze],\n",
    "                    \"lower\": np.mean([s for s in sCFR_lowers if len(s)>0], axis=0)[:T_analyze],\n",
    "                    \"upper\": np.mean([s for s in sCFR_uppers if len(s)>0], axis=0)[:T_analyze],\n",
    "                    \"cf_mean\": np.mean([s for s in sCFR_cf_means if len(s)>0], axis=0)[:T_analyze],\n",
    "                    \"cf_lower\": np.mean([s for s in sCFR_cf_lowers if len(s)>0], axis=0)[:T_analyze],\n",
    "                    \"cf_upper\": np.mean([s for s in sCFR_cf_uppers if len(s)>0], axis=0)[:T_analyze]\n",
    "                },\n",
    "                \"cCFR_cumulative\": {\n",
    "                    \"mean\": np.mean(cCFR_means, axis=0)[:T_analyze],\n",
    "                    \"lower\": np.mean(cCFR_lowers, axis=0)[:T_analyze],\n",
    "                    \"upper\": np.mean(cCFR_uppers, axis=0)[:T_analyze]\n",
    "                },\n",
    "                \"aCFR_cumulative\": {\n",
    "                    \"mean\": np.mean(aCFR_means, axis=0)[:T_analyze],\n",
    "                    \"lower\": np.mean(aCFR_lowers, axis=0)[:T_analyze],\n",
    "                    \"upper\": np.mean(aCFR_uppers, axis=0)[:T_analyze]\n",
    "                },\n",
    "                \"ITS_MLE\": {\n",
    "                    \"factual_mean\": np.mean(ITS_factual_means, axis=0)[:T_analyze],\n",
    "                    \"factual_lower\": np.mean(ITS_factual_lowers, axis=0)[:T_analyze],\n",
    "                    \"factual_upper\": np.mean(ITS_factual_uppers, axis=0)[:T_analyze],\n",
    "                    \"cf_mean\": np.mean(ITS_cf_means, axis=0)[:T_analyze],\n",
    "                    \"cf_lower\": np.mean(ITS_cf_lowers, axis=0)[:T_analyze],\n",
    "                    \"cf_upper\": np.mean(ITS_cf_uppers, axis=0)[:T_analyze]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        aggregated_plot_data_list.append(agg_plot_dict)\n",
    "\n",
    "    return aggregated_plot_data_list\n",
    "\n",
    "def main_analysis():\n",
    "    \"\"\"Main function to orchestrate the post-hoc analysis of simulation results.\"\"\"\n",
    "    all_loaded_metrics_list = []\n",
    "    \n",
    "    print(\"Starting analysis of existing simulation results...\")\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    for dir_path in [config.OUTPUT_DIR_PLOTS, config.OUTPUT_DIR_TABLES, config.OUTPUT_DIR_RESULTS_CSV]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # --- Load all saved metrics ---\n",
    "    for scenario_idx, scenario_config_dict in enumerate(tqdm(config.SCENARIOS, desc=\"Loading All Metrics\")):\n",
    "        scenario_id = scenario_config_dict[\"id\"]\n",
    "        for mc_run_idx in range(config.NUM_MONTE_CARLO_RUNS):\n",
    "            run_metrics = results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "            if run_metrics:\n",
    "                all_loaded_metrics_list.append(run_metrics)\n",
    "    \n",
    "    if not all_loaded_metrics_list:\n",
    "        print(\"No metrics files found. Cannot generate plots or tables.\")\n",
    "        return\n",
    "        \n",
    "    results_df_all = pd.DataFrame(all_loaded_metrics_list)\n",
    "    results_df_valid = sanitize_metrics_dataframe(results_df_all)\n",
    "    results_df_valid = results_df_valid[results_df_valid['error'].isin([None, \"None\"])].copy()\n",
    "    \n",
    "    if results_df_valid.empty:\n",
    "        print(\"No valid simulation runs found. Analysis cannot proceed.\")\n",
    "        return\n",
    "    \n",
    "    # --- Prepare data for plotting and tables ---\n",
    "    cover_cols = [col for col in results_df_valid.columns if 'cover' in col]\n",
    "    for col in cover_cols:\n",
    "        results_df_valid[col] = results_df_valid[col].astype('Int64')\n",
    "\n",
    "    summary_metrics_mean = results_df_valid.groupby(\"scenario_id\").mean(numeric_only=True).reset_index()\n",
    "    summary_metrics_std = results_df_valid.groupby(\"scenario_id\").std(numeric_only=True).reset_index()\n",
    "    \n",
    "    summary_metrics_mean = summary_metrics_mean.add_suffix('_mean').rename(columns={'scenario_id_mean':'scenario_id'})\n",
    "    summary_metrics_std = summary_metrics_std.add_suffix('_std').rename(columns={'scenario_id_std':'scenario_id'})\n",
    "    results_df_summary_for_tables = pd.merge(summary_metrics_mean, summary_metrics_std, on=\"scenario_id\", how=\"left\")\n",
    "    \n",
    "    analysis_csv_path = os.path.join(config.OUTPUT_DIR_RESULTS_CSV, \"all_scenarios_metrics_aggregated.csv\")\n",
    "    results_df_summary_for_tables.to_csv(analysis_csv_path, index=False)\n",
    "    print(f\"\\nAggregated summary metrics saved to {analysis_csv_path}\")\n",
    "\n",
    "    # --- Generate Plots and Tables ---\n",
    "    print(\"\\nPreparing aggregated data for summary plot...\")\n",
    "    aggregated_plot_data = prepare_aggregated_plot_data(results_df_all)\n",
    "    \n",
    "    print(\"Generating combined 4x3 aggregated summary plot...\")\n",
    "    plotting.plot_aggregated_scenarios_summary(aggregated_plot_data, config.OUTPUT_DIR_PLOTS)\n",
    "    \n",
    "    print(\"Generating summary boxplots...\")\n",
    "    plotting.plot_metric_summary_boxplots(results_df_valid, config.OUTPUT_DIR_PLOTS)\n",
    "    \n",
    "    print(\"Generating LaTeX summary tables...\")\n",
    "    tables.generate_rt_metrics_table(results_df_summary_for_tables, config.OUTPUT_DIR_TABLES)\n",
    "    tables.generate_param_metrics_table(results_df_summary_for_tables, config.OUTPUT_DIR_TABLES)\n",
    "\n",
    "    print(\"\\nAnalysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91dee7e-862f-45ba-b182-ee0f2e357196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_loaded_metrics_list = []\n",
    "\n",
    "# print(\"Starting analysis of existing simulation results...\")\n",
    "\n",
    "# # Create output directories if they don't exist\n",
    "# for dir_path in [config.OUTPUT_DIR_PLOTS, config.OUTPUT_DIR_TABLES, config.OUTPUT_DIR_RESULTS_CSV]:\n",
    "#     os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# # --- Load all saved metrics ---\n",
    "# for scenario_idx, scenario_config_dict in enumerate(tqdm(config.SCENARIOS, desc=\"Loading All Metrics\")):\n",
    "#     scenario_id = scenario_config_dict[\"id\"]\n",
    "#     for mc_run_idx in range(config.NUM_MONTE_CARLO_RUNS):\n",
    "#         run_metrics = results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "#         if run_metrics:\n",
    "#             all_loaded_metrics_list.append(run_metrics)\n",
    "\n",
    "# results_df_all = pd.DataFrame(all_loaded_metrics_list)\n",
    "# results_df_valid = sanitize_metrics_dataframe(results_df_all)\n",
    "# results_df_valid = results_df_valid[results_df_valid['error'].isin([None, \"None\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29c32b6-cbb9-41ee-aabe-a7fa5311fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated_plot_data_list = []\n",
    "# study_global_seed = config.GLOBAL_BASE_SEED\n",
    "\n",
    "# for scenario_idx, scenario_config_dict in enumerate(tqdm(config.SCENARIOS, desc=\"Aggregating Plot Data\")):\n",
    "#     scenario_id = scenario_config_dict[\"id\"]\n",
    "#     scenario_base_seed = study_global_seed + (scenario_idx * config.NUM_MONTE_CARLO_RUNS * 1000)\n",
    "    \n",
    "#     sim_data_true = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=scenario_base_seed)\n",
    "#     T_analyze = config.T_ANALYSIS_LENGTH\n",
    "\n",
    "#     scen_df_valid = results_df_all[(results_df_all[\"scenario_id\"] == scenario_id) & (results_df_all[\"error\"].isin([None, \"None\"]))]\n",
    "#     if scen_df_valid.empty: continue\n",
    "    \n",
    "#     # Stacks for collecting time-series data from all valid runs\n",
    "#     sCFR_O_means, sCFR_O_lowers, sCFR_O_uppers = [], [], []\n",
    "#     sCFR_O_cf_means, sCFR_O_cf_lowers, sCFR_O_cf_uppers = [], [], []\n",
    "#     cCFR_means, cCFR_lowers, cCFR_uppers = [], [], []\n",
    "#     aCFR_means, aCFR_lowers, aCFR_uppers = [], [], []\n",
    "#     ITS_factual_means, ITS_factual_lowers, ITS_factual_uppers = [], [], []\n",
    "#     ITS_cf_means, ITS_cf_lowers, ITS_cf_uppers = [], [], []\n",
    "\n",
    "#     for mc_run_idx in scen_df_valid[\"mc_run\"].astype(int) - 1:\n",
    "#         # Load posterior summary for the superior sCFR-O model for plotting\n",
    "#         posterior_summary = results_io.load_posterior_summary_for_run(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SUMMARIES, model_name=\"sCFR_O\")\n",
    "#         if posterior_summary:\n",
    "#             sCFR_O_means.append(posterior_summary.get(\"p_mean\", []))\n",
    "#             sCFR_O_lowers.append(posterior_summary.get(\"p_q025\", []))\n",
    "#             sCFR_O_uppers.append(posterior_summary.get(\"p_q975\", []))\n",
    "#             sCFR_O_cf_means.append(posterior_summary.get(\"p_cf_mean\", []))\n",
    "#             sCFR_O_cf_lowers.append(posterior_summary.get(\"p_cf_q025\", []))\n",
    "#             sCFR_O_cf_uppers.append(posterior_summary.get(\"p_cf_q975\", []))\n",
    "\n",
    "#         print(\"sCFR_O:\",sCFR_O_means)\n",
    "\n",
    "#         # Regenerate data to calculate benchmarks for this run\n",
    "#         sim_data_run = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=(scenario_base_seed + mc_run_idx))\n",
    "#         benchmark_cis = benchmarks.calculate_benchmark_cis_with_bayesian(sim_data_run[\"d_t\"], sim_data_run[\"c_t\"], sim_data_run[\"f_s_true\"])\n",
    "#         its_results = benchmarks.calculate_its_with_parametric_bootstrap(\n",
    "#             sim_data_run[\"d_t\"], sim_data_run[\"c_t\"], sim_data_run[\"f_s_true\"],\n",
    "#             sim_data_run[\"Bm_true\"], sim_data_run[\"true_intervention_times_0_abs\"], sim_data_run[\"beta_signs_true\"]\n",
    "#         )\n",
    "        \n",
    "#         cCFR_means.append(benchmarks.calculate_crude_cfr(sim_data_run[\"d_t\"], sim_data_run[\"c_t\"], cumulative=True))\n",
    "#         cCFR_lowers.append(benchmark_cis[\"cCFR_cumulative_lower\"])\n",
    "#         cCFR_uppers.append(benchmark_cis[\"cCFR_cumulative_upper\"])\n",
    "        \n",
    "#         aCFR_means.append(benchmarks.calculate_nishiura_cfr_cumulative(sim_data_run[\"d_t\"], sim_data_run[\"c_t\"], sim_data_run[\"f_s_true\"]))\n",
    "#         aCFR_lowers.append(benchmark_cis[\"aCFR_cumulative_lower\"])\n",
    "#         aCFR_uppers.append(benchmark_cis[\"aCFR_cumulative_upper\"])\n",
    "        \n",
    "#         ITS_factual_means.append(its_results[\"its_factual_mean\"])\n",
    "#         ITS_factual_lowers.append(its_results[\"its_factual_lower\"])\n",
    "#         ITS_factual_uppers.append(its_results[\"its_factual_upper\"])\n",
    "#         ITS_cf_means.append(its_results[\"its_counterfactual_mean\"])\n",
    "#         ITS_cf_lowers.append(its_results[\"its_counterfactual_lower\"])\n",
    "#         ITS_cf_uppers.append(its_results[\"its_counterfactual_upper\"])\n",
    "\n",
    "#     # Calculate the point-wise average of the curves and intervals\n",
    "#     agg_plot_dict = {\n",
    "#         \"scenario_id\": scenario_id,\n",
    "#         \"true_r_t\": sim_data_true[\"true_r_0_t\"][:T_analyze],\n",
    "#         \"true_rcf_0_t\": sim_data_true[\"true_rcf_0_t\"][:T_analyze],\n",
    "#         \"estimated_r_t_dict\": {\n",
    "#             \"sCFR-O\": {\n",
    "#                 \"mean\": np.mean([s for s in sCFR_O_means if len(s)>0], axis=0)[:T_analyze],\n",
    "#                 \"lower\": np.mean([s for s in sCFR_O_lowers if len(s)>0], axis=0)[:T_analyze],\n",
    "#                 \"upper\": np.mean([s for s in sCFR_O_uppers if len(s)>0], axis=0)[:T_analyze],\n",
    "#                 \"cf_mean\": np.mean([s for s in sCFR_O_cf_means if len(s)>0], axis=0)[:T_analyze],\n",
    "#                 \"cf_lower\": np.mean([s for s in sCFR_O_cf_lowers if len(s)>0], axis=0)[:T_analyze],\n",
    "#                 \"cf_upper\": np.mean([s for s in sCFR_O_cf_uppers if len(s)>0], axis=0)[:T_analyze]\n",
    "#             },\n",
    "#             \"cCFR_cumulative\": {\n",
    "#                 \"mean\": np.mean(cCFR_means, axis=0)[:T_analyze],\n",
    "#                 \"lower\": np.mean(cCFR_lowers, axis=0)[:T_analyze],\n",
    "#                 \"upper\": np.mean(cCFR_uppers, axis=0)[:T_analyze]\n",
    "#             },\n",
    "#             \"aCFR_cumulative\": {\n",
    "#                 \"mean\": np.mean(aCFR_means, axis=0)[:T_analyze],\n",
    "#                 \"lower\": np.mean(aCFR_lowers, axis=0)[:T_analyze],\n",
    "#                 \"upper\": np.mean(aCFR_uppers, axis=0)[:T_analyze]\n",
    "#             },\n",
    "#             \"ITS_MLE\": {\n",
    "#                 \"factual_mean\": np.mean(ITS_means, axis=0)[:T_analyze],\n",
    "#                 \"factual_lower\": np.mean(ITS_lowers, axis=0)[:T_analyze],\n",
    "#                 \"factual_upper\": np.mean(ITS_uppers, axis=0)[:T_analyze],\n",
    "#                 \"cf_mean\": np.mean(ITS_cf_means, axis=0)[:T_analyze],\n",
    "#                 \"cf_lower\": np.mean(ITS_cf_lowers, axis=0)[:T_analyze],\n",
    "#                 \"cf_upper\": np.mean(ITS_cf_uppers, axis=0)[:T_analyze]\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "#     aggregated_plot_data_list.append(agg_plot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1ac744-83f7-42af-967e-735e4388e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Main function to orchestrate the post-hoc analysis of simulation results.\"\"\"\n",
    "# all_loaded_metrics_list = []\n",
    "\n",
    "# print(\"Starting analysis of existing simulation results...\")\n",
    "\n",
    "# # Create output directories if they don't exist\n",
    "# for dir_path in [config.OUTPUT_DIR_PLOTS, config.OUTPUT_DIR_TABLES, config.OUTPUT_DIR_RESULTS_CSV]:\n",
    "#     os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# # --- Load all saved metrics ---\n",
    "# for scenario_idx, scenario_config_dict in enumerate(tqdm(config.SCENARIOS, desc=\"Loading All Metrics\")):\n",
    "#     scenario_id = scenario_config_dict[\"id\"]\n",
    "#     for mc_run_idx in range(config.NUM_MONTE_CARLO_RUNS):\n",
    "#         run_metrics = results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "#         if run_metrics:\n",
    "#             all_loaded_metrics_list.append(run_metrics)\n",
    "\n",
    "# results_df_all = pd.DataFrame(all_loaded_metrics_list)\n",
    "# results_df_valid = sanitize_metrics_dataframe(results_df_all)\n",
    "# results_df_valid = results_df_valid[results_df_valid['error'].isin([None, \"None\"])].copy()\n",
    "\n",
    "# # --- Prepare data for plotting and tables ---\n",
    "# cover_cols = [col for col in results_df_valid.columns if 'cover' in col]\n",
    "# for col in cover_cols:\n",
    "#     results_df_valid[col] = results_df_valid[col].astype('Int64')\n",
    "\n",
    "# summary_metrics_mean = results_df_valid.groupby(\"scenario_id\").mean(numeric_only=True).reset_index()\n",
    "# summary_metrics_std = results_df_valid.groupby(\"scenario_id\").std(numeric_only=True).reset_index()\n",
    "\n",
    "# summary_metrics_mean = summary_metrics_mean.add_suffix('_mean').rename(columns={'scenario_id_mean':'scenario_id'})\n",
    "# summary_metrics_std = summary_metrics_std.add_suffix('_std').rename(columns={'scenario_id_std':'scenario_id'})\n",
    "# results_df_summary_for_tables = pd.merge(summary_metrics_mean, summary_metrics_std, on=\"scenario_id\", how=\"left\")\n",
    "\n",
    "# analysis_csv_path = os.path.join(config.OUTPUT_DIR_RESULTS_CSV, \"all_scenarios_metrics_aggregated.csv\")\n",
    "# results_df_summary_for_tables.to_csv(analysis_csv_path, index=False)\n",
    "# print(f\"\\nAggregated summary metrics saved to {analysis_csv_path}\")\n",
    "\n",
    "# # --- Generate Plots and Tables ---\n",
    "# print(\"\\nPreparing aggregated data for summary plot...\")\n",
    "# aggregated_plot_data = prepare_aggregated_plot_data(results_df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa8bc82-d889-43a1-bfa4-86dda550c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.plot_aggregated_scenarios_summary(aggregated_plot_data, config.OUTPUT_DIR_PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d50821f2-1692-4407-a915-192e18a8fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Main function to orchestrate the post-hoc analysis of simulation results.\"\"\"\n",
    "# all_loaded_metrics_list = []\n",
    "\n",
    "# print(\"Starting analysis of existing simulation results...\")\n",
    "\n",
    "# # Create output directories if they don't exist\n",
    "# for dir_path in [config.OUTPUT_DIR_PLOTS, config.OUTPUT_DIR_TABLES, config.OUTPUT_DIR_RESULTS_CSV]:\n",
    "#     os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# # --- Load all saved metrics ---\n",
    "# for scenario_idx, scenario_config_dict in enumerate(tqdm(config.SCENARIOS, desc=\"Loading All Metrics\")):\n",
    "#     scenario_id = scenario_config_dict[\"id\"]\n",
    "#     for mc_run_idx in range(config.NUM_MONTE_CARLO_RUNS):\n",
    "#         run_metrics = results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "#         if run_metrics:\n",
    "#             all_loaded_metrics_list.append(run_metrics)\n",
    "\n",
    "# results_df_all = pd.DataFrame(all_loaded_metrics_list)\n",
    "# results_df_valid = sanitize_metrics_dataframe(results_df_all)\n",
    "# results_df_valid = results_df_valid[results_df_valid['error'].isin([None, \"None\"])].copy()\n",
    "\n",
    "# # --- Prepare data for plotting and tables ---\n",
    "# cover_cols = [col for col in results_df_valid.columns if 'cover' in col]\n",
    "# for col in cover_cols:\n",
    "#     results_df_valid[col] = results_df_valid[col].astype('Int64')\n",
    "\n",
    "# summary_metrics_mean = results_df_valid.groupby(\"scenario_id\").mean(numeric_only=True).reset_index()\n",
    "# summary_metrics_std = results_df_valid.groupby(\"scenario_id\").std(numeric_only=True).reset_index()\n",
    "\n",
    "# summary_metrics_mean = summary_metrics_mean.add_suffix('_mean').rename(columns={'scenario_id_mean':'scenario_id'})\n",
    "# summary_metrics_std = summary_metrics_std.add_suffix('_std').rename(columns={'scenario_id_std':'scenario_id'})\n",
    "# results_df_summary_for_tables = pd.merge(summary_metrics_mean, summary_metrics_std, on=\"scenario_id\", how=\"left\")\n",
    "\n",
    "# analysis_csv_path = os.path.join(config.OUTPUT_DIR_RESULTS_CSV, \"all_scenarios_metrics_aggregated.csv\")\n",
    "# results_df_summary_for_tables.to_csv(analysis_csv_path, index=False)\n",
    "# print(f\"\\nAggregated summary metrics saved to {analysis_csv_path}\")\n",
    "\n",
    "# # --- Generate Plots and Tables ---\n",
    "# print(\"\\nPreparing aggregated data for summary plot...\")\n",
    "# aggregated_plot_data = prepare_aggregated_plot_data(results_df_all)\n",
    "\n",
    "# print(\"Generating combined 4x3 aggregated summary plot...\")\n",
    "# plotting.plot_aggregated_scenarios_summary(aggregated_plot_data, config.OUTPUT_DIR_PLOTS)\n",
    "\n",
    "# print(\"Generating summary boxplots...\")\n",
    "# plotting.plot_metric_summary_boxplots(results_df_valid, config.OUTPUT_DIR_PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca76c19-5d9b-4961-b7f5-fa14b9776dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df_summary_for_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f47a54b-343a-44d0-922c-4c6c5574440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables.generate_rt_metrics_table(results_df_summary_for_tables, config.OUTPUT_DIR_TABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d63a8",
   "metadata": {},
   "source": [
    "## 3. Execute Analysis\n",
    "\n",
    "Run the cell below to perform the analysis. Make sure the simulation output directories in `config.py` point to where your simulation results are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48d7007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of existing simulation results...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3604fa79be24a06ad429b4e2dd9afb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading All Metrics:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregated summary metrics saved to ./simulation_outputs/results_csv/all_scenarios_metrics_aggregated.csv\n",
      "\n",
      "Preparing aggregated data for summary plot...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075b51995ef346218147f54840679d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aggregating Plot Data:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating combined 4x3 aggregated summary plot...\n",
      "Generating summary boxplots...\n",
      "Generating LaTeX summary tables...\n",
      "Summary table for r_t metrics saved to ./simulation_outputs/tables/summary_table_rt_metrics.csv\n",
      "LaTeX table for r_t metrics saved to ./simulation_outputs/tables/summary_table_rt_metrics.tex\n",
      "Summary table for sCFR parameter metrics saved to ./simulation_outputs/tables/summary_table_param_metrics_sCFR.csv\n",
      "LaTeX table for sCFR parameter metrics saved to ./simulation_outputs/tables/summary_table_param_metrics_sCFR.tex\n",
      "Summary table for ITS parameter metrics saved to ./simulation_outputs/tables/summary_table_param_metrics_ITS.csv\n",
      "LaTeX table for ITS parameter metrics saved to ./simulation_outputs/tables/summary_table_param_metrics_ITS.tex\n",
      "\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b294836-3edd-44a2-8fcf-bef88e2ca2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
