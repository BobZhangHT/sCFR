{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73a71ee",
   "metadata": {},
   "source": [
    "# Simulation Study Runner\n",
    "\n",
    "This notebook executes the main simulation study. It performs Monte Carlo simulations for 12 defined scenarios in parallel, leveraging multiple CPU cores. \n",
    "\n",
    "**Key Features:**\n",
    "- **Parallel Processing:** Uses `multiprocessing` to speed up Monte Carlo runs.\n",
    "- **Checkpointing:** Skips computation for (scenario, MC run) combinations if results (metrics JSON files or raw posterior samples) are already found. This allows resuming interrupted simulations.\n",
    "- **Output Saving:** Saves detailed results for each run:\n",
    "    - Raw MCMC posterior samples (`.npz` files) to `OUTPUT_DIR_POSTERIOR_SAMPLES`.\n",
    "    - Summaries of posteriors (mean, median, quantiles as `.npz` files) to `OUTPUT_DIR_POSTERIOR_SUMMARIES`.\n",
    "    - Calculated evaluation metrics (`.json` files) to `OUTPUT_DIR_RUN_METRICS_JSON`.\n",
    "- Aggregated metrics per scenario are also saved as CSV files to `OUTPUT_DIR_RESULTS_CSV`.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Ensure `sampler.py` is modified to accept `rng_key` in its `sample` function (as detailed above).\n",
    "2. All helper Python modules (`config.py`, `data_generation.py`, `benchmarks.py`, `model_fitting.py`, `evaluation.py`, `results_io.py`) must be in the same directory or accessible in `PYTHONPATH`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95a269",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da72ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 cores for parallel processing.\n",
      "Global base seed for the study: 2025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import jax \n",
    "import json\n",
    "from joblib import Parallel, delayed # For parallel processing\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# Custom modules\n",
    "import config\n",
    "import data_generation\n",
    "import benchmarks\n",
    "import model_fitting\n",
    "import evaluation\n",
    "import results_io \n",
    "\n",
    "print(f\"Using {config.NUM_CORES_TO_USE} cores for parallel processing.\")\n",
    "print(f\"Global base seed for the study: {config.GLOBAL_BASE_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664316e4",
   "metadata": {},
   "source": [
    "## 2. Worker Function for Single Monte Carlo Replication\n",
    "\n",
    "This function encapsulates all operations for one Monte Carlo run. It includes checkpointing. Plot data is not returned by this worker in this version; `analyze_results_notebook.ipynb` will handle plot data generation from saved summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6e76c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_single_mc_replication(args_tuple):\n",
    "#     \"\"\"\n",
    "#     Executes one full Monte Carlo replication for a given scenario.\n",
    "\n",
    "#     This function is designed to be called by joblib.Parallel. It handles data generation, \n",
    "#     model fitting, metric calculation, and saving results for a single run, including \n",
    "#     checkpointing to skip re-computation if results already exist.\n",
    "\n",
    "#     Args:\n",
    "#         args_tuple (tuple): A tuple containing the arguments for the worker.\n",
    "#             - scenario_config_dict (dict): Configuration for the specific scenario.\n",
    "#             - mc_run_idx (int): The index of the current Monte Carlo run (e.g., 0 to N-1).\n",
    "#             - scenario_base_seed (int): The base seed for this scenario's set of runs.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (metrics_dictionary, None). plot_data is None as plotting is handled later.\n",
    "#     \"\"\"\n",
    "\n",
    "#     scenario_config_dict, mc_run_idx, scenario_base_seed = args_tuple\n",
    "#     scenario_id = scenario_config_dict[\"id\"]\n",
    "    \n",
    "#     metrics_filepath = os.path.join(\n",
    "#         config.OUTPUT_DIR_RUN_METRICS_JSON,\n",
    "#         f\"metrics_scen_{scenario_id}_run_{mc_run_idx+1}.json\"\n",
    "#     )\n",
    "\n",
    "#     if not config.OVERWRITE_EXISTING_RESULTS and os.path.exists(metrics_filepath):\n",
    "#         loaded_metrics = results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "#         if loaded_metrics and loaded_metrics.get(\"error\") in [None, \"None\"]:\n",
    "#             return loaded_metrics, None\n",
    "    \n",
    "#     run_specific_seed_dgp = scenario_base_seed + mc_run_idx \n",
    "#     jax_prng_key = jax.random.PRNGKey(run_specific_seed_dgp + 100000)\n",
    "#     current_run_metrics = {\"scenario_id\": scenario_id, \"mc_run\": mc_run_idx + 1, \"error\": \"None\"}\n",
    "    \n",
    "#     try:\n",
    "#         sim_data = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=run_specific_seed_dgp)\n",
    "        \n",
    "#         # --- Model Fitting with separate checkpointing for raw samples ---\n",
    "#         posterior_samples_scfr = results_io.load_raw_posterior_samples(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR\")\n",
    "#         if posterior_samples_scfr is None:\n",
    "#             posterior_samples_scfr, _ = model_fitting.fit_proposed_model(sim_data, jax_prng_key, use_orthogonalization=False)\n",
    "#             results_io.save_raw_posterior_samples(scenario_id, mc_run_idx, posterior_samples_scfr, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR\")\n",
    "        \n",
    "#         posterior_samples_scfr_o = None\n",
    "#         if config.COMPARE_SCFR_AND_SCFR_O:\n",
    "#             posterior_samples_scfr_o = results_io.load_raw_posterior_samples(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR_O\")\n",
    "#             if posterior_samples_scfr_o is None:\n",
    "#                 jax_prng_key, subkey = jax.random.split(jax_prng_key)\n",
    "#                 posterior_samples_scfr_o, _ = model_fitting.fit_proposed_model(sim_data, subkey, use_orthogonalization=True)\n",
    "#                 results_io.save_raw_posterior_samples(scenario_id, mc_run_idx, posterior_samples_scfr_o, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR_O\")\n",
    "        \n",
    "#         # --- Calculate benchmarks and all metrics ---\n",
    "#         benchmark_r_t_estimates = {\n",
    "#             \"cCFR_cumulative\": benchmarks.calculate_crude_cfr(sim_data[\"d_t\"], sim_data[\"c_t\"], cumulative=True),\n",
    "#             \"aCFR_cumulative\": benchmarks.calculate_nishiura_cfr_cumulative(sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"])\n",
    "#         }\n",
    "#         benchmark_cis = benchmarks.calculate_benchmark_cis_with_bayesian(sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"])\n",
    "#         its_results = benchmarks.calculate_its_with_parametric_bootstrap(\n",
    "#             d_t=sim_data[\"d_t\"],\n",
    "#             c_t=sim_data[\"c_t\"],\n",
    "#             f_s=sim_data[\"f_s_true\"],\n",
    "#             Bm=sim_data[\"Bm_true\"],\n",
    "#             intervention_times_abs=sim_data[\"true_intervention_times_0_abs\"],\n",
    "#             intervention_signs=sim_data[\"beta_signs_true\"]\n",
    "#         )\n",
    "        \n",
    "#         calculated_metrics = evaluation.collect_all_metrics(\n",
    "#             sim_data, posterior_samples_scfr, posterior_samples_scfr_o, \n",
    "#             benchmark_r_t_estimates, benchmark_cis, its_results\n",
    "#         )\n",
    "#         current_run_metrics.update(calculated_metrics)\n",
    "#         current_run_metrics[\"status\"] = \"computed\"\n",
    "#         results_io.save_run_metrics(scenario_id, mc_run_idx, current_run_metrics, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "        \n",
    "#     # scenario_config_dict, mc_run_idx, scenario_base_seed = args_tuple\n",
    "#     # scenario_id = scenario_config_dict[\"id\"]\n",
    "\n",
    "#     # metrics_filepath = os.path.join(\n",
    "#     #     config.OUTPUT_DIR_RUN_METRICS_JSON,\n",
    "#     #     f\"metrics_scen_{scenario_id}_run_{mc_run_idx+1}.json\"\n",
    "#     # )\n",
    "#     # raw_posterior_samples_filepath = os.path.join(\n",
    "#     #     config.OUTPUT_DIR_POSTERIOR_SAMPLES,\n",
    "#     #     f\"scen_{scenario_id}_run_{mc_run_idx+1}_posterior_samples.npz\"\n",
    "#     # )\n",
    "\n",
    "#     # # --- Checkpoint: If final metrics file exists for a successful run, skip ---\n",
    "#     # if not config.OVERWRITE_EXISTING_RESULTS and os.path.exists(metrics_filepath):\n",
    "#     #     loaded_metrics = results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "#     #     if loaded_metrics and loaded_metrics.get(\"error\") in [None, \"None\"]:\n",
    "#     #         return loaded_metrics, None\n",
    "    \n",
    "#     # run_specific_seed_dgp = scenario_base_seed + mc_run_idx \n",
    "#     # jax_prng_key = jax.random.PRNGKey(run_specific_seed_dgp + 100000)\n",
    "#     # current_run_metrics = {\"scenario_id\": scenario_id, \"mc_run\": mc_run_idx + 1, \"error\": \"None\"}\n",
    "    \n",
    "#     # try:\n",
    "#     #     # Checkpoint: Load existing raw samples if MCMC was already run\n",
    "#     #     sim_data = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=run_specific_seed_dgp)\n",
    "        \n",
    "#     #     # --- Model Fitting with separate checkpointing for raw samples ---\n",
    "#     #     posterior_samples_scfr = results_io.load_raw_posterior_samples(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR\")\n",
    "#     #     if posterior_samples_scfr is None:\n",
    "#     #         posterior_samples_scfr, _ = model_fitting.fit_proposed_model(sim_data, jax_prng_key, use_orthogonalization=False)\n",
    "#     #         results_io.save_raw_posterior_samples(scenario_id, mc_run_idx, posterior_samples_scfr, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR\")\n",
    "        \n",
    "#     #     posterior_samples_scfr_o = None\n",
    "#     #     if config.COMPARE_SCFR_AND_SCFR_O:\n",
    "#     #         posterior_samples_scfr_o = results_io.load_raw_posterior_samples(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR_O\")\n",
    "#     #         if posterior_samples_scfr_o is None:\n",
    "#     #             jax_prng_key, subkey = jax.random.split(jax_prng_key)\n",
    "#     #             posterior_samples_scfr_o, _ = model_fitting.fit_proposed_model(sim_data, subkey, use_orthogonalization=True)\n",
    "#     #             results_io.save_raw_posterior_samples(scenario_id, mc_run_idx, posterior_samples_scfr_o, config.OUTPUT_DIR_POSTERIOR_SAMPLES, model_name=\"sCFR_O\")\n",
    "        \n",
    "#     #     # posterior_samples = None\n",
    "#     #     # if os.path.exists(raw_posterior_samples_filepath):\n",
    "#     #     #     posterior_samples = results_io.load_raw_posterior_samples(scenario_id, mc_run_idx, config.OUTPUT_DIR_POSTERIOR_SAMPLES)\n",
    "#     #     #     current_run_metrics[\"status\"] = \"loaded_raw_samples\"\n",
    "        \n",
    "#     #     # # Regenerate data deterministically for this run to get true values for metrics\n",
    "#     #     # sim_data = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=run_specific_seed_dgp)\n",
    "\n",
    "#     #     # posterior_samples_scfr = None\n",
    "#     #     # posterior_samples_scfr_o = None\n",
    "        \n",
    "#     #     # # Fit standard sCFR model\n",
    "#     #     # print(f\"    -> Fitting standard sCFR model...\")\n",
    "#     #     # posterior_samples_scfr, _ = model_fitting.fit_proposed_model(sim_data, jax_prng_key, use_orthogonalization=False)\n",
    "#     #     # results_io.save_posterior_summary_for_run(scenario_id, mc_run_idx, posterior_samples_scfr, config.OUTPUT_DIR_POSTERIOR_SUMMARIES, model_name=\"sCFR\")\n",
    "\n",
    "#     #     # if config.COMPARE_SCFR_AND_SCFR_O:\n",
    "#     #     #     # Fit orthogonalized sCFR-O model\n",
    "#     #     #     print(f\"    -> Fitting orthogonalized sCFR-O model...\")\n",
    "#     #     #     jax_prng_key, subkey = jax.random.split(jax_prng_key)\n",
    "#     #     #     posterior_samples_scfr_o, _ = model_fitting.fit_proposed_model(sim_data, subkey, use_orthogonalization=True)\n",
    "#     #     #     results_io.save_posterior_summary_for_run(scenario_id, mc_run_idx, posterior_samples_scfr_o, config.OUTPUT_DIR_POSTERIOR_SUMMARIES, model_name=\"sCFR_O\")\n",
    "\n",
    "#     #     # # If samples were not loaded, run the MCMC model fitting\n",
    "#     #     # if posterior_samples is None: \n",
    "#     #     #     posterior_samples, _ = model_fitting.fit_proposed_model(sim_data, jax_prng_key_mcmc)\n",
    "#     #     #     current_run_metrics[\"status\"] = \"computed_mcmc\"\n",
    "#     #     #     results_io.save_raw_posterior_samples(scenario_id, mc_run_idx, posterior_samples, config.OUTPUT_DIR_POSTERIOR_SAMPLES)\n",
    "        \n",
    "#     #     # # Save posterior summary (mean, quantiles) from raw samples\n",
    "#     #     # results_io.save_posterior_summary_for_run(\n",
    "#     #     #     scenario_id, mc_run_idx, posterior_samples, config.OUTPUT_DIR_POSTERIOR_SUMMARIES\n",
    "#     #     # )\n",
    "            \n",
    "#     #     # Calculate benchmarks and metrics\n",
    "#     #     benchmark_r_t_estimates = {\n",
    "#     #         \"cCFR_cumulative\": benchmarks.calculate_crude_cfr(sim_data[\"d_t\"], sim_data[\"c_t\"], cumulative=True),\n",
    "#     #         \"aCFR_cumulative\": benchmarks.calculate_nishiura_cfr_cumulative(sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"])\n",
    "#     #     }\n",
    "        \n",
    "#     #     # Call the new Bayesian CI function (fast, no loop)\n",
    "#     #     benchmark_cis = benchmarks.calculate_benchmark_cis_with_bayesian(\n",
    "#     #         sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"]\n",
    "#     #     )\n",
    "\n",
    "#     #     # NEW: Calculate ITS benchmark results\n",
    "#     #     its_results = benchmarks.calculate_its_with_parametric_bootstrap(\n",
    "#     #         d_t=sim_data[\"d_t\"],\n",
    "#     #         c_t=sim_data[\"c_t\"],\n",
    "#     #         f_s=sim_data[\"f_s_true\"],\n",
    "#     #         Bm=sim_data[\"Bm_true\"],\n",
    "#     #         intervention_times_abs=sim_data[\"true_intervention_times_0_abs\"],\n",
    "#     #         intervention_signs=sim_data[\"beta_signs_true\"]\n",
    "#     #     )\n",
    "\n",
    "#     #     # --- Calculate Evaluation Metrics (pass the new benchmark_cis) ---\n",
    "#     #     # calculated_metrics = evaluation.collect_all_metrics(\n",
    "#     #     #     sim_data, posterior_samples, benchmark_r_t_estimates, benchmark_cis\n",
    "#     #     # )\n",
    "#     #     # calculated_metrics = evaluation.collect_all_metrics(\n",
    "#     #     #     sim_data, posterior_samples, benchmark_r_t_estimates, benchmark_cis, its_results\n",
    "#     #     # )\n",
    "#     #     calculated_metrics = evaluation.collect_all_metrics(\n",
    "#     #         sim_data, posterior_samples_scfr, posterior_samples_scfr_o, \n",
    "#     #         benchmark_r_t_estimates, benchmark_cis, its_results\n",
    "#     #     )\n",
    "#     #     current_run_metrics.update(calculated_metrics)\n",
    "#     #     current_run_metrics[\"status\"] = \"computed\"\n",
    "#     #     results_io.save_run_metrics(scenario_id, mc_run_idx, current_run_metrics, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "        \n",
    "\n",
    "#     except Exception as e:\n",
    "#         import traceback\n",
    "#         error_message = f\"ERROR in WORKER {os.getpid()} for Scen {scenario_id}, Run {mc_run_idx + 1}: {type(e).__name__} - {e}\\n{traceback.format_exc()}\"\n",
    "#         print(f\"\\n---!!! {error_message} !!!---\\n\") # Make error more visible\n",
    "#         current_run_metrics[\"error\"] = error_message\n",
    "#         current_run_metrics[\"status\"] = \"error_in_run\"\n",
    "#         try:\n",
    "#             results_io.save_run_metrics(scenario_id, mc_run_idx, current_run_metrics, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "#         except Exception as save_e:\n",
    "#             print(f\"    WORKER {os.getpid()}: CRITICAL ERROR - Could not save error metrics: {save_e}\")\n",
    "            \n",
    "#     return current_run_metrics, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb65048d-440b-466a-8e2a-b2bd4ef6eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_mc_replication(args_tuple):\n",
    "    \"\"\"Executes one full Monte Carlo replication for a given scenario.\"\"\"\n",
    "    scenario_config_dict, mc_run_idx, scenario_base_seed = args_tuple\n",
    "    scenario_id = scenario_config_dict[\"id\"]\n",
    "    \n",
    "    metrics_filepath = os.path.join(\n",
    "        config.OUTPUT_DIR_RUN_METRICS_JSON,\n",
    "        f\"metrics_scen_{scenario_id}_run_{mc_run_idx+1}.json\"\n",
    "    )\n",
    "\n",
    "    if not config.OVERWRITE_EXISTING_RESULTS and os.path.exists(metrics_filepath):\n",
    "        loaded_metrics = results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "        if loaded_metrics and loaded_metrics.get(\"error\") in [None, \"None\"]:\n",
    "            return loaded_metrics, None\n",
    "    \n",
    "    run_specific_seed_dgp = scenario_base_seed + mc_run_idx \n",
    "    jax_prng_key = jax.random.PRNGKey(run_specific_seed_dgp + 100000)\n",
    "    current_run_metrics = {\"scenario_id\": scenario_id, \"mc_run\": mc_run_idx + 1, \"error\": \"None\"}\n",
    "    \n",
    "    try:\n",
    "        sim_data = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=run_specific_seed_dgp)\n",
    "        \n",
    "        # Fit the standard sCFR model\n",
    "        posterior_samples_scfr, _ = model_fitting.fit_proposed_model(sim_data, jax_prng_key)\n",
    "        results_io.save_posterior_summary_for_run(scenario_id, mc_run_idx, posterior_samples_scfr, config.OUTPUT_DIR_POSTERIOR_SUMMARIES)\n",
    "\n",
    "        # Benchmark Calculations\n",
    "        benchmark_r_t_estimates = {\n",
    "            \"cCFR_cumulative\": benchmarks.calculate_crude_cfr(sim_data[\"d_t\"], sim_data[\"c_t\"], cumulative=True),\n",
    "            \"aCFR_cumulative\": benchmarks.calculate_nishiura_cfr_cumulative(sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"])\n",
    "        }\n",
    "        benchmark_cis = benchmarks.calculate_benchmark_cis_with_bayesian(sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"])\n",
    "        \n",
    "        its_results = benchmarks.calculate_its_with_penalized_mle(\n",
    "            d_t=sim_data[\"d_t\"], c_t=sim_data[\"c_t\"], f_s=sim_data[\"f_s_true\"],\n",
    "            Bm=sim_data[\"Bm_true\"],\n",
    "            intervention_times_abs=sim_data[\"true_intervention_times_0_abs\"],\n",
    "            intervention_signs=sim_data[\"beta_signs_true\"]\n",
    "        )\n",
    "\n",
    "        all_benchmark_results = {**benchmark_r_t_estimates, **benchmark_cis, **its_results}\n",
    "        results_io.save_benchmark_results(scenario_id, mc_run_idx, all_benchmark_results, config.OUTPUT_DIR_BENCHMARK_RESULTS)\n",
    "        \n",
    "        # Collect all scalar metrics\n",
    "        calculated_metrics = evaluation.collect_all_metrics(\n",
    "            sim_data, posterior_samples_scfr,\n",
    "            benchmark_r_t_estimates, benchmark_cis, its_results\n",
    "        )\n",
    "        current_run_metrics.update(calculated_metrics)\n",
    "        results_io.save_run_metrics(scenario_id, mc_run_idx, current_run_metrics, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_message = f\"ERROR in WORKER: {e}\\n{traceback.format_exc()}\"\n",
    "        current_run_metrics[\"error\"] = error_message\n",
    "        results_io.save_run_metrics(scenario_id, mc_run_idx, current_run_metrics, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "            \n",
    "    return current_run_metrics, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9e60f",
   "metadata": {},
   "source": [
    "## 3. Main Simulation Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ddcffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_simulation_runner():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the entire simulation study.\n",
    "    It iterates through all defined scenarios and uses a joblib Parallel backend \n",
    "    to execute all Monte Carlo runs with real-time progress bars.\n",
    "    \"\"\"\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    # Create all output directories\n",
    "    # for dir_path in [config.OUTPUT_DIR_PLOTS, config.OUTPUT_DIR_TABLES, \n",
    "    #                  config.OUTPUT_DIR_RESULTS_CSV, config.OUTPUT_DIR_POSTERIOR_SUMMARIES,\n",
    "    #                  config.OUTPUT_DIR_POSTERIOR_SAMPLES, config.OUTPUT_DIR_RUN_METRICS_JSON]:\n",
    "    for dir_path in [config.OUTPUT_DIR_TABLES, config.OUTPUT_DIR_RESULTS_CSV, config.OUTPUT_DIR_POSTERIOR_SUMMARIES,\n",
    "                     config.OUTPUT_DIR_BENCHMARK_RESULTS,config.OUTPUT_DIR_POSTERIOR_SAMPLES, config.OUTPUT_DIR_RUN_METRICS_JSON]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    all_results_metrics_list = []\n",
    "    with tqdm(total=len(config.SCENARIOS), desc=\"Overall Scenario Progress\") as outer_pbar:\n",
    "        for scenario_idx, scenario_config_dict in enumerate(config.SCENARIOS):\n",
    "            scenario_id = scenario_config_dict[\"id\"]\n",
    "            outer_pbar.set_description(f\"Processing Scen: {scenario_id}\")\n",
    "            \n",
    "            scenario_base_seed = config.GLOBAL_BASE_SEED + (scenario_idx * config.NUM_MONTE_CARLO_RUNS * 1000)\n",
    "            mc_args_list = [(scenario_config_dict, i, scenario_base_seed) for i in range(config.NUM_MONTE_CARLO_RUNS)]\n",
    "\n",
    "            try:\n",
    "                # Use joblib Parallel for execution with a nested tqdm progress bar\n",
    "                scenario_run_outputs = Parallel(n_jobs=config.NUM_CORES_TO_USE)(\n",
    "                    delayed(run_single_mc_replication)(args) for args in tqdm(\n",
    "                        mc_args_list, desc=f\"  MC Runs for {scenario_id}\", leave=False, position=1\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                current_scenario_metrics_list = [result[0] for result in scenario_run_outputs]\n",
    "                \n",
    "                if current_scenario_metrics_list:\n",
    "                    df_scenario_metrics = pd.DataFrame(current_scenario_metrics_list)\n",
    "                    csv_path = os.path.join(config.OUTPUT_DIR_RESULTS_CSV, f\"metrics_scen_{scenario_id}.csv\")\n",
    "                    df_scenario_metrics.to_csv(csv_path, index=False)\n",
    "                    all_results_metrics_list.extend(current_scenario_metrics_list)\n",
    "            \n",
    "            except Exception as e_pool:\n",
    "                print(f\"\\nCritical error during parallel processing for scenario {scenario_id}: {e_pool}\")\n",
    "            \n",
    "            outer_pbar.update(1)\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    print(f\"\\nAll Monte Carlo simulations completed in {end_time_total - start_time_total:.2f} seconds.\")\n",
    "\n",
    "    # Save combined results\n",
    "    if all_results_metrics_list:\n",
    "        results_df_all = pd.DataFrame(all_results_metrics_list)\n",
    "        results_df_all.to_csv(os.path.join(config.OUTPUT_DIR_RESULTS_CSV, \"all_scenarios_metrics_combined.csv\"), index=False)\n",
    "        print(f\"Combined metrics saved.\")\n",
    "    \n",
    "    print(\"\\nSimulation runs complete. Use analyze_results.py to generate plots and final tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9dbc2",
   "metadata": {},
   "source": [
    "## 4. Execute Simulation\n",
    "\n",
    "The following cell will start the simulation process. Ensure all configurations in `config.py` are set as desired and `sampler.py` has been modified for dynamic `rng_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c47fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scen: S01:   0%|                                                                     | 0/12 [00:00<?, ?it/s]\n",
      "\u001b[AC Runs for S01:   0%|                                                                        | 0/10 [00:00<?, ?it/s]\n",
      "Processing Scen: S01:   0%|                                                                     | 0/12 [00:03<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__': # This idiom is helpful if you convert notebook to .py\n",
    "#     # Important for multiprocessing on Windows, and good practice elsewhere\n",
    "# multiprocessing.freeze_support() \n",
    "main_simulation_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865ffc7-78b3-40ca-85f8-75ff9c801ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189444bc-b5b7-48d6-86a4-4aedf9e8e77a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
