{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Study Runner\n",
    "\n",
    "This notebook executes the main simulation study. It performs Monte Carlo simulations for all defined scenarios in parallel, leveraging multiple CPU cores and displaying progress with `tqdm` progress bars.\n",
    "\n",
    "**Key Features:**\n",
    "- **Parallel Processing:** Uses `joblib` to speed up Monte Carlo runs.\n",
    "- **Progress Bars:** Provides real-time feedback on the simulation progress.\n",
    "- **Checkpointing:** Skips computation for runs where results are already found, allowing simulations to be resumed.\n",
    "- **Comprehensive Output:** Saves raw MCMC samples, posterior summaries, benchmark results, and evaluation metrics for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5 cores for parallel processing.\n",
      "Global base seed for the study: 2025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import jax \n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom modules\n",
    "import config\n",
    "import data_generation\n",
    "import benchmarks\n",
    "import model_fitting\n",
    "import evaluation\n",
    "import results_io \n",
    "\n",
    "print(f\"Using {config.NUM_CORES_TO_USE} cores for parallel processing.\")\n",
    "print(f\"Global base seed for the study: {config.GLOBAL_BASE_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Worker Function for a Single Monte Carlo Run\n",
    "\n",
    "This function encapsulates all operations for one simulation run. It is designed to be called in parallel by `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_mc_replication(args_tuple):\n",
    "    \"\"\"Executes one full Monte Carlo replication for a given scenario.\"\"\"\n",
    "    scenario_config_dict, mc_run_idx, scenario_base_seed = args_tuple\n",
    "    scenario_id = scenario_config_dict[\"id\"]\n",
    "    \n",
    "    # --- Checkpoint: Skip if results already exist and overwriting is disabled ---\n",
    "    if not config.OVERWRITE_EXISTING_RESULTS:\n",
    "        if results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON):\n",
    "            return results_io.load_run_metrics(scenario_id, mc_run_idx, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "    \n",
    "    run_specific_seed = scenario_base_seed + mc_run_idx \n",
    "    jax_prng_key = jax.random.PRNGKey(run_specific_seed + 100000)\n",
    "    current_run_metrics = {\"scenario_id\": scenario_id, \"mc_run\": mc_run_idx + 1, \"error\": \"None\"}\n",
    "    \n",
    "    try:\n",
    "        # 1. Generate Data\n",
    "        sim_data = data_generation.simulate_scenario_data(scenario_config_dict, run_seed=run_specific_seed)\n",
    "        \n",
    "        # 2. Fit the proposed sCFR model\n",
    "        posterior_samples_scfr, _ = model_fitting.fit_proposed_model(sim_data, jax_prng_key)\n",
    "        \n",
    "        # 3. Save raw samples and summaries for the sCFR model\n",
    "        results_io.save_raw_posterior_samples(scenario_id, mc_run_idx, posterior_samples_scfr, config.OUTPUT_DIR_POSTERIOR_SAMPLES)\n",
    "        results_io.save_posterior_summary_for_run(scenario_id, mc_run_idx, posterior_samples_scfr, config.OUTPUT_DIR_POSTERIOR_SUMMARIES)\n",
    "\n",
    "        # 4. Run all benchmark models\n",
    "        benchmark_r_t_estimates = {\n",
    "            \"cCFR_cumulative\": benchmarks.calculate_crude_cfr(sim_data[\"d_t\"], sim_data[\"c_t\"], cumulative=True),\n",
    "            \"aCFR_cumulative\": benchmarks.calculate_nishiura_cfr_cumulative(sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"])\n",
    "        }\n",
    "        benchmark_cis = benchmarks.calculate_benchmark_cis_with_bayesian(sim_data[\"d_t\"], sim_data[\"c_t\"], sim_data[\"f_s_true\"])\n",
    "        its_results = benchmarks.calculate_its_with_penalized_mle(\n",
    "            d_t=sim_data[\"d_t\"], c_t=sim_data[\"c_t\"], f_s=sim_data[\"f_s_true\"],\n",
    "            Bm=sim_data[\"Bm_true\"],\n",
    "            intervention_times_abs=sim_data[\"true_intervention_times_0_abs\"],\n",
    "            intervention_signs=sim_data[\"beta_signs_true\"]\n",
    "        )\n",
    "\n",
    "        all_benchmark_results = {**benchmark_r_t_estimates, **benchmark_cis, **its_results}\n",
    "        results_io.save_benchmark_results(scenario_id, mc_run_idx, all_benchmark_results, config.OUTPUT_DIR_BENCHMARK_RESULTS)\n",
    "        \n",
    "        # 5. Collect all scalar evaluation metrics\n",
    "        calculated_metrics = evaluation.collect_all_metrics(\n",
    "            sim_data, \n",
    "            posterior_samples_scfr,\n",
    "            benchmark_r_t_estimates, \n",
    "            benchmark_cis, \n",
    "            its_results\n",
    "        )\n",
    "        current_run_metrics.update(calculated_metrics)\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        error_message = f\"ERROR in WORKER: {e}\\n{traceback.format_exc()}\"\n",
    "        current_run_metrics[\"error\"] = error_message\n",
    "    \n",
    "    # 6. Save the final metrics for this run\n",
    "    results_io.save_run_metrics(scenario_id, mc_run_idx, current_run_metrics, config.OUTPUT_DIR_RUN_METRICS_JSON)\n",
    "    return current_run_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Simulation Execution Loop\n",
    "\n",
    "This function orchestrates the study, iterating through all scenarios and executing the Monte Carlo runs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_simulation_runner():\n",
    "    \"\"\"Main function to run the entire simulation study with progress bars.\"\"\"\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    # Create all necessary output directories\n",
    "    for dir_path in [config.OUTPUT_DIR_TABLES, config.OUTPUT_DIR_RESULTS_CSV, \n",
    "                     config.OUTPUT_DIR_POSTERIOR_SUMMARIES, config.OUTPUT_DIR_BENCHMARK_RESULTS,\n",
    "                     config.OUTPUT_DIR_POSTERIOR_SAMPLES, config.OUTPUT_DIR_RUN_METRICS_JSON]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    all_results_metrics_list = []\n",
    "    \n",
    "    # Outer loop for scenarios with a progress bar\n",
    "    for scenario_idx, scenario_config in enumerate(tqdm(config.SCENARIOS, desc=\"Overall Scenario Progress\")):\n",
    "        scenario_id = scenario_config[\"id\"]\n",
    "        scenario_base_seed = config.GLOBAL_BASE_SEED + (scenario_idx * config.NUM_MONTE_CARLO_RUNS * 100)\n",
    "        \n",
    "        # Prepare arguments for each Monte Carlo run\n",
    "        mc_args_list = [(scenario_config, i, scenario_base_seed) for i in range(config.NUM_MONTE_CARLO_RUNS)]\n",
    "\n",
    "        try:\n",
    "            # Use joblib for parallel execution with a nested tqdm progress bar\n",
    "            scenario_metrics = Parallel(n_jobs=config.NUM_CORES_TO_USE)(\n",
    "                delayed(run_single_mc_replication)(args) for args in tqdm(\n",
    "                    mc_args_list, desc=f\"MC Runs for {scenario_id}\", leave=False\n",
    "                )\n",
    "            )\n",
    "            all_results_metrics_list.extend(scenario_metrics)\n",
    "        \n",
    "        except Exception as e_pool:\n",
    "            print(f\"\\nCRITICAL ERROR during parallel processing for scenario {scenario_id}: {e_pool}\")\n",
    "\n",
    "    end_time_total = time.time()\n",
    "    print(f\"\\nAll simulations completed in {end_time_total - start_time_total:.2f} seconds.\")\n",
    "\n",
    "    # Save a final combined CSV of all metrics\n",
    "    if all_results_metrics_list:\n",
    "        results_df_all = pd.DataFrame([m for m in all_results_metrics_list if m is not None])\n",
    "        results_df_all.to_csv(os.path.join(config.OUTPUT_DIR_RESULTS_CSV, \"all_scenarios_metrics_combined.csv\"), index=False)\n",
    "        print(\"Combined metrics for all runs saved.\")\n",
    "    \n",
    "    print(\"\\nSimulation runs complete. You can now use the analysis notebook to generate plots and tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute Simulation\n",
    "\n",
    "The following cell will start the simulation process. Ensure all configurations in `config.py` are set as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Scenario Progress:   0%|                                                                | 0/12 [00:00<?, ?it/s]\n",
      "\u001b[ARuns for S01:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[ARuns for S01: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 44.23it/s]\n",
      "Overall Scenario Progress:   8%|████▋                                                   | 1/12 [00:40<07:30, 40.91s/it]\n",
      "\u001b[ARuns for S02:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  17%|█████████▎                                              | 2/12 [02:35<14:02, 84.21s/it]\n",
      "\u001b[ARuns for S03:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  25%|█████████████▊                                         | 3/12 [05:39<19:28, 129.89s/it]\n",
      "\u001b[ARuns for S04:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  33%|██████████████████▋                                     | 4/12 [06:20<12:39, 94.92s/it]\n",
      "\u001b[ARuns for S05:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  42%|██████████████████████▉                                | 5/12 [08:30<12:31, 107.39s/it]\n",
      "\u001b[ARuns for S06:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  50%|███████████████████████████▌                           | 6/12 [11:03<12:18, 123.07s/it]\n",
      "\u001b[ARuns for S07:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  58%|████████████████████████████████                       | 7/12 [11:58<08:22, 100.59s/it]\n",
      "\u001b[ARuns for S08:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  67%|████████████████████████████████████▋                  | 8/12 [14:04<07:14, 108.70s/it]\n",
      "\u001b[ARuns for S09:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[AD:\\Users\\user\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "Overall Scenario Progress:  75%|█████████████████████████████████████████▎             | 9/12 [17:40<07:06, 142.23s/it]\n",
      "\u001b[ARuns for S10:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  83%|█████████████████████████████████████████████         | 10/12 [18:45<03:57, 118.52s/it]\n",
      "\u001b[ARuns for S11:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress:  92%|█████████████████████████████████████████████████▌    | 11/12 [20:49<02:00, 120.16s/it]\n",
      "\u001b[ARuns for S12:   0%|                                                                           | 0/5 [00:00<?, ?it/s]\n",
      "Overall Scenario Progress: 100%|██████████████████████████████████████████████████████| 12/12 [23:57<00:00, 119.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All simulations completed in 1437.44 seconds.\n",
      "Combined metrics for all runs saved.\n",
      "\n",
      "Simulation runs complete. You can now use the analysis notebook to generate plots and tables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main_simulation_runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
